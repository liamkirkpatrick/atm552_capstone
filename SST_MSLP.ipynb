{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2672ae12-294e-4db3-afd9-e145d4ba8faa",
   "metadata": {},
   "source": [
    "# Global Mean EOF Analysis\n",
    "Liam Kirkpatrick\n",
    "ATM 552, Assignment 5\n",
    "February 4, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9b788-65c2-4e92-bd03-cbc61bf46891",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This code is modeled after Dennis Hartman's script on github (https://github.com/dennislhartmann/Objective_Analysis/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c699526-b1cf-4f74-813d-e8a292abb492",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6e54aa-25e4-4798-9ba8-10bb158f2d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import netcdf\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.signal as sig\n",
    "import xarray as xr\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "from cartopy import config\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# land mask package\n",
    "from global_land_mask import globe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ddfab-8724-46c6-80ea-a3803049cca2",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "This should be the only place we need to change things for the rest of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0642bbf7-fb95-446c-9719-54da1668109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #  Global domain\n",
    "    lat1=88.\n",
    "    lat2=-88.\n",
    "    lon1=0.\n",
    "    lon2=360.\n",
    "\n",
    "if True:\n",
    "    # Traditional PDO region\n",
    "    lat1=65.\n",
    "    lat2=20.\n",
    "    lon1=120.\n",
    "    lon2=260.\n",
    "\n",
    "# set years to begin and end for climatology\n",
    "yrbeg = 1940\n",
    "yrend = 2023\n",
    "\n",
    "#set the years for eof analysis\n",
    "yra = yrbeg\n",
    "yrb = yrend\n",
    "\n",
    "# central longitude for Robinson maps\n",
    "cent_lon = -160.   \n",
    "\n",
    "# behind the scenes bookkeeping on year selection\n",
    "ds_yrmin = 1940 # min year of dataset\n",
    "iyr1 = (yrbeg-ds_yrmin)*12\n",
    "iyr2 = (yrend - ds_yrmin + 1)*12\n",
    "\n",
    "# choices on removal\n",
    "rem_globmean = True  # removes global mean anomaly from each grid cell\n",
    "rem_ann = True    # removes annual cycle from each grid cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7114f516-ee52-4dd8-83dd-24d1feb300b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't read index file '../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib.923a8.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Liam/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py\", line 545, in from_indexpath_or_filestream\n",
      "    filestream_mtime = os.path.getmtime(filestream.path)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Import Data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# open using xarray\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ds_disk \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib\u001b[39m\u001b[38;5;124m'\u001b[39m,engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfgrib\u001b[39m\u001b[38;5;124m'\u001b[39m,) \n\u001b[1;32m      5\u001b[0m sst \u001b[38;5;241m=\u001b[39m ds_disk\u001b[38;5;241m.\u001b[39msst\n\u001b[1;32m      6\u001b[0m mslp \u001b[38;5;241m=\u001b[39m ds_disk\u001b[38;5;241m.\u001b[39mmsl\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/xarray/backends/api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 566\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    567\u001b[0m     filename_or_obj,\n\u001b[1;32m    568\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    573\u001b[0m     backend_ds,\n\u001b[1;32m    574\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/xarray_plugin.py:108\u001b[0m, in \u001b[0;36mCfGribBackend.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, lock, indexpath, filter_by_keys, read_keys, encode_cf, squeeze, time_dims, errors, extra_coords)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     filename_or_obj: T\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mstr\u001b[39m, abc\u001b[38;5;241m.\u001b[39mMappingFieldset[T\u001b[38;5;241m.\u001b[39mAny, abc\u001b[38;5;241m.\u001b[39mField]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     extra_coords: T\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[0;32m--> 108\u001b[0m     store \u001b[38;5;241m=\u001b[39m CfGribDataStore(\n\u001b[1;32m    109\u001b[0m         filename_or_obj,\n\u001b[1;32m    110\u001b[0m         indexpath\u001b[38;5;241m=\u001b[39mindexpath,\n\u001b[1;32m    111\u001b[0m         filter_by_keys\u001b[38;5;241m=\u001b[39mfilter_by_keys,\n\u001b[1;32m    112\u001b[0m         read_keys\u001b[38;5;241m=\u001b[39mread_keys,\n\u001b[1;32m    113\u001b[0m         encode_cf\u001b[38;5;241m=\u001b[39mencode_cf,\n\u001b[1;32m    114\u001b[0m         squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[1;32m    115\u001b[0m         time_dims\u001b[38;5;241m=\u001b[39mtime_dims,\n\u001b[1;32m    116\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m    117\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    118\u001b[0m         extra_coords\u001b[38;5;241m=\u001b[39mextra_coords,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclose_on_error(store):\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28mvars\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mload()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/xarray_plugin.py:40\u001b[0m, in \u001b[0;36mCfGribDataStore.__init__\u001b[0;34m(self, filename, lock, **backend_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     opener \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mopen_fieldset\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m opener(filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/dataset.py:782\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(path, grib_errors, indexpath, filter_by_keys, read_keys, time_dims, extra_coords, **kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m stream \u001b[38;5;241m=\u001b[39m messages\u001b[38;5;241m.\u001b[39mFileStream(path, errors\u001b[38;5;241m=\u001b[39mgrib_errors)\n\u001b[1;32m    781\u001b[0m index_keys \u001b[38;5;241m=\u001b[39m compute_index_keys(time_dims, extra_coords)\n\u001b[0;32m--> 782\u001b[0m index \u001b[38;5;241m=\u001b[39m open_fileindex(stream, indexpath, index_keys, filter_by_keys\u001b[38;5;241m=\u001b[39mfilter_by_keys)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m open_from_index(index, read_keys, time_dims, extra_coords, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/dataset.py:761\u001b[0m, in \u001b[0;36mopen_fileindex\u001b[0;34m(stream, indexpath, index_keys, filter_by_keys, computed_keys)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_fileindex\u001b[39m(\n\u001b[1;32m    754\u001b[0m     stream: messages\u001b[38;5;241m.\u001b[39mFileStream,\n\u001b[1;32m    755\u001b[0m     indexpath: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m messages\u001b[38;5;241m.\u001b[39mDEFAULT_INDEXPATH,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m     computed_keys: messages\u001b[38;5;241m.\u001b[39mComputedKeysType \u001b[38;5;241m=\u001b[39m cfmessage\u001b[38;5;241m.\u001b[39mCOMPUTED_KEYS,\n\u001b[1;32m    759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m messages\u001b[38;5;241m.\u001b[39mFileIndex:\n\u001b[1;32m    760\u001b[0m     index_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_keys) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(filter_by_keys))\n\u001b[0;32m--> 761\u001b[0m     index \u001b[38;5;241m=\u001b[39m messages\u001b[38;5;241m.\u001b[39mFileIndex\u001b[38;5;241m.\u001b[39mfrom_indexpath_or_filestream(\n\u001b[1;32m    762\u001b[0m         stream, index_keys, indexpath\u001b[38;5;241m=\u001b[39mindexpath, computed_keys\u001b[38;5;241m=\u001b[39mcomputed_keys\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index\u001b[38;5;241m.\u001b[39msubindex(filter_by_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py:561\u001b[0m, in \u001b[0;36mFileIndex.from_indexpath_or_filestream\u001b[0;34m(cls, filestream, index_keys, indexpath, computed_keys, log)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read index file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, indexpath)\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_fieldset(filestream, index_keys, computed_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py:378\u001b[0m, in \u001b[0;36mFieldsetIndex.from_fieldset\u001b[0;34m(cls, fieldset, index_keys, computed_keys)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     iteritems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(fieldset)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py:391\u001b[0m, in \u001b[0;36mFieldsetIndex.from_fieldset_and_iteritems\u001b[0;34m(cls, fieldset, iteritems, index_keys, computed_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m index_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(index_keys)\n\u001b[1;32m    390\u001b[0m header_values_cache \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# type: T.Dict[T.Tuple[T.Any, type], T.Any]\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m field_id, raw_field \u001b[38;5;129;01min\u001b[39;00m iteritems:\n\u001b[1;32m    392\u001b[0m     field \u001b[38;5;241m=\u001b[39m ComputedKeysAdapter(raw_field, computed_keys)\n\u001b[1;32m    393\u001b[0m     header_values \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py:291\u001b[0m, in \u001b[0;36mFileStreamItems.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m old_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    290\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitervalues():\n\u001b[1;32m    292\u001b[0m     offset \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmessage_get(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;241m==\u001b[39m old_offset:\n",
      "File \u001b[0;32m~/anaconda3/envs/atm552/lib/python3.11/site-packages/cfgrib/messages.py:267\u001b[0m, in \u001b[0;36mFileStreamItems.itervalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitervalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T\u001b[38;5;241m.\u001b[39mIterator[Message]:\n\u001b[1;32m    266\u001b[0m     errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilestream\u001b[38;5;241m.\u001b[39merrors\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilestream\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# enable MULTI-FIELD support on sequential reads (like when building the index)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m multi_enabled(file):\n\u001b[1;32m    270\u001b[0m             valid_message_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib'"
     ]
    }
   ],
   "source": [
    "## Import Data\n",
    "\n",
    "# open using xarray\n",
    "ds_disk = xr.open_dataset('../data/adaptor.mars.internal-1708137618.9186294-22048-4-1f8f03ca-bdfd-48d7-93b6-88f79b1175d3.grib',engine='cfgrib',) \n",
    "sst = ds_disk.sst\n",
    "mslp = ds_disk.msl\n",
    "lon = ds_disk.longitude\n",
    "lat = ds_disk.latitude\n",
    "\n",
    "# uncomment next line to see data format.\n",
    "#ds_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c169f9-8d1f-4296-bd9e-b9b8bd2c4741",
   "metadata": {},
   "source": [
    "## Clean up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2136e9-af76-41ab-a1cf-a64fa7aca258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend longitude array\n",
    "lonp = np.empty(len(lon)+1)\n",
    "lonp[0:len(lon)] = lon\n",
    "lonp[len(lon)]= lon[len(lon)-1]+lon[1]-lon[0]\n",
    "\n",
    "# Calculate SST and MSLP Mean\n",
    "sstm = np.nanmean(sst,0)\n",
    "mslpm = np.nanmean(mslp,0)\n",
    "\n",
    "# Here we are adding cyclic continuity to SST, so it plots without gaps\n",
    "sstmp = np.empty([len(lat),len(lon)+1])\n",
    "sstmp[:,0:len(lon)]= sstm\n",
    "sstmp[:,len(lon+1)]= sstm[:,0]\n",
    "\n",
    "# Here we are adding cyclic continuity to MSLP, so it plots without gaps\n",
    "mslpmp = np.empty([len(lat),len(lon)+1])\n",
    "mslpmp[:,0:len(lon)]= mslpm\n",
    "mslpmp[:,len(lon+1)]= mslpm[:,0]\n",
    "\n",
    "# clip out years I want to consider\n",
    "sst1 = sst[iyr1:iyr2,:,:]\n",
    "mslp1 = mslp[iyr1:iyr2,:,:]\n",
    "print('sst clipped shape ',np.shape(sst))\n",
    "print('mslp clipped shape ',np.shape(mslp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369099e-7b02-4ae3-b890-e9a77c36c3e8",
   "metadata": {},
   "source": [
    "## Make Grand Mean SST Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44333d-bd9b-42cb-a6bd-11fe18220aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_mapc = 'RdYlBu_r'\n",
    "col_mapr = 'RdYlBu_r'\n",
    "\n",
    "# make figure - SST\n",
    "plt.figure(figsize=(12, 4), dpi=100)\n",
    "\n",
    "# set up axis. Use cent_lon set above\n",
    "ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "\n",
    "# add color for sst\n",
    "plt.contourf(lonp, lat, sstmp, 60, cmap=col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "\n",
    "# Housekeeping\n",
    "plt.title('Grand Mean SST')\n",
    "ax.coastlines()\n",
    "plt.grid()\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bf0371-604a-41c2-8994-98d1d75a2813",
   "metadata": {},
   "source": [
    "## Make grand mean MSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fc5d3-97d3-4df3-9a11-e7d392fcc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figure - MSLP\n",
    "plt.figure(figsize=(12, 4), dpi=100)\n",
    "\n",
    "# set up axis. Use cent_lon set above\n",
    "ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "\n",
    "# add color for mslp\n",
    "plt.contourf(lonp, lat, mslpmp, 60, cmap=col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "\n",
    "# Housekeeping\n",
    "plt.title('Grand Mean MSLP')\n",
    "ax.coastlines()\n",
    "plt.grid()\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9785a-275b-4877-91a8-c9be79efeef5",
   "metadata": {},
   "source": [
    "## Remove Annual Cycle\n",
    "\n",
    "Here we remove the annual cycle, deleting the influence of seasonality from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912b6c3-2472-4446-971b-0321d1282f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition to numpy space\n",
    "sstx = xr.DataArray.to_numpy(sst1)\n",
    "mslpx = xr.DataArray.to_numpy(mslp1)\n",
    "\n",
    "# extract latitude and longitude\n",
    "latx=len(lat) \n",
    "lonx=len(lon)\n",
    "\n",
    "# extract shape of sstx\n",
    "sts = np.shape(sstx)\n",
    "yrmx = int(sts[0]/12)\n",
    "\n",
    "# Now remove annual cycle from sst and mslp.\n",
    "#First, reshape:\n",
    "sstx = np.reshape(sstx,(yrmx,12,latx,lonx))\n",
    "mslpx = np.reshape(mslpx,(yrmx,12,latx,lonx))\n",
    "\n",
    "# Now calculate monthly mean\n",
    "sstm = np.nanmean(sstx,0)\n",
    "mslpm = np.nanmean(mslpx,0)\n",
    "\n",
    "if rem_ann:\n",
    "    # subtract mean from each month\n",
    "    for i in range(0,yrmx):\n",
    "        sstx[i,:,:,:] = sstx[i,:,:,:]-sstm\n",
    "        mslpx[i,:,:,:] = mslpx[i,:,:,:]-mslpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9d262-9ab8-4650-91ac-f89ed49efb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sstm)\n",
    "720/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefab051-0d50-4c8d-87b0-1c2a4df0620b",
   "metadata": {},
   "source": [
    "## Plot a monthly mean - SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018bb89-0125-42cc-8b18-82f5bf87c6e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# let's try to plot a monthly mean\n",
    "\n",
    "# Plot 1 - January\n",
    "if True:\n",
    "    # make figure\n",
    "    plt.figure(figsize=(12, 4), dpi=100)\n",
    "    # Set up projection\n",
    "    ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "    # plot data\n",
    "    plt.contourf(lon, lat, sstm[0,:,:], 60, cmap = col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "    # add gridlines\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "    # housekeeping\n",
    "    ax.coastlines()\n",
    "    plt.colorbar()\n",
    "    plt.title('Climatological mean SST for January \\N{DEGREE SIGN}C')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# PLOT 2 - July\n",
    "if True:\n",
    "    # make figure\n",
    "    plt.figure(figsize=(12, 4), dpi=100)\n",
    "    # Set up projection\n",
    "    ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "    # plot data\n",
    "    plt.contourf(lon, lat, sstm[6,:,:], 60, cmap = col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "    # add gridlines\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "    # housekeeping\n",
    "    ax.coastlines()\n",
    "    plt.colorbar()\n",
    "    plt.title('Climatological mean SST for July \\N{DEGREE SIGN}K')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52bf53-6d16-4ea8-a939-11269733b91a",
   "metadata": {},
   "source": [
    "## Plot a monthly mean - MSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784113a-bbb0-4639-9b6a-dcffb227f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to plot a monthly mean\n",
    "\n",
    "# Plot 1 - January\n",
    "if True:\n",
    "    # make figure\n",
    "    plt.figure(figsize=(12, 4), dpi=100)\n",
    "    # Set up projection\n",
    "    ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "    # plot data\n",
    "    plt.contourf(lon, lat, mslpm[0,:,:], 60, cmap = col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "    # add gridlines\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "    # housekeeping\n",
    "    ax.coastlines()\n",
    "    plt.colorbar()\n",
    "    plt.title('Climatological mean MSLP for January ')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# PLOT 2 - July\n",
    "if True:\n",
    "    # make figure\n",
    "    plt.figure(figsize=(12, 4), dpi=100)\n",
    "    # Set up projection\n",
    "    ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "    # plot data\n",
    "    plt.contourf(lon, lat, mslpm[6,:,:], 60, cmap = col_mapc,\n",
    "             transform=ccrs.PlateCarree())\n",
    "    # add gridlines\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "    # housekeeping\n",
    "    ax.coastlines()\n",
    "    plt.colorbar()\n",
    "    plt.title('Climatological mean MSLP for July')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3ac34-3746-40e7-9b8b-bfc8259d780a",
   "metadata": {},
   "source": [
    "## Pick subset of months / reigon for EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bad2c-5385-4e6f-8aa7-5efef4087097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Specify the geographical boundaries and months we want to use\n",
    "# let's experiment a bit with the xarray version\n",
    "#sst  # this statement tells you what is in the structure sst \n",
    "#ssta = sst.sel(time=slice(1950-1-1, 2022-1-1),lon=slice(0., 90.),lat=slice(0., 89.))\n",
    "\n",
    "\n",
    "# Specify year range, print for user confirmation\n",
    "yr1= str(yra)+'-01-01'\n",
    "yr2= str(yrb)+'-12-31'\n",
    "years = np.linspace(float(yra),float(yrb+1),num = (yrb-yra+1)*12)\n",
    "print('size years',np.shape(years),yr1,yr2)\n",
    "\n",
    "# select all data\n",
    "sst0 = sst.sel(time=slice(yr1, yr2),latitude=slice(90., -90),longitude=slice(0., 360.))\n",
    "mslp0 = mslp.sel(time=slice(yr1, yr2),latitude=slice(90., -90),longitude=slice(0., 360.))\n",
    "\n",
    "# select data in specified range\n",
    "ssta = sst.sel(time=slice(yr1, yr2),latitude=slice(lat1, lat2),longitude=slice(lon1, lon2))\n",
    "mslpa = mslp.sel(time=slice(yr1, yr2),latitude=slice(lat1, lat2),longitude=slice(lon1, lon2))\n",
    "\n",
    "# make arrays of lat and lon\n",
    "lat_sh=ssta.latitude\n",
    "lon_sh=ssta.longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332b0d9-3691-4181-93eb-2bda099c68cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7be403-7881-4990-a7b0-169127b224fc",
   "metadata": {},
   "source": [
    "## Remove Global Mean. Weight by sqrt(cosine(latitude)) so area variance is used in EOF computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de55bcd4-d15a-4164-92a4-4fab5fe47502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vector for weigting the data array for sqrt(cosine(latitude)) so area variance is used in EOF computation\n",
    "cn = np.sqrt(np.cos(lat_sh*np.pi/180.))\n",
    "\n",
    "# Apply weighting vector (for use in calculating the global mean)\n",
    "ssta_weighted = ssta * 0\n",
    "for i in range(len(lat_sh)):\n",
    "    # multiply all entries in this row by the weight in the weights vector (cn)\n",
    "    ssta_weighted[:,i,:] = ssta[:,i,:]*cn[i]  # times sqrt(cos(lat))\n",
    "print(\"Weighting Complete\")\n",
    "    \n",
    "# calculate global mean anomaly\n",
    "gm = []\n",
    "for i in range(len(years)):\n",
    "    gm.append(np.nanmean(ssta_weighted[i,:,:]))\n",
    "\n",
    "# Apply global mean / area weighting\n",
    "if rem_globmean:\n",
    "    \n",
    "    # first remove global mean\n",
    "    for i in range(len(years)):\n",
    "        ssta[i,:,:] = ssta[i,:,:] - gm[i]\n",
    "\n",
    "    # next apply latitude weighting\n",
    "    for i in range(len(lat_sh)):\n",
    "    # multiply all entries in this row by the weight in the weights vector (cn)\n",
    "        ssta[:,i,:] = ssta[:,i,:]*cn[i]  # times sqrt(cos(lat))\n",
    "\n",
    "else:\n",
    "    # if we aren't removing global mean, we still need to apply latitude weighting\n",
    "    ssta = ssta_weighted\n",
    "    \n",
    "# plot global mean anomaly\n",
    "if True:\n",
    "    fig,axs = plt.subplots()\n",
    "    axs.set_ylabel('Global Mean SST Anomaly ($^\\circ$C)')\n",
    "    axs.set_xlabel('Time')\n",
    "    axs.set_title('Global Mean SST Anomaly (monthly)')\n",
    "    axs.plot(years,gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a088086-57b3-4f12-b61d-f1a7fc88c833",
   "metadata": {},
   "source": [
    "## Test plot to make sure  all is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c5ceb-abe3-4c51-b0b5-43741eda3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    # pick arbitrary month\n",
    "    i = 150\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.contourf(lon_sh,lat_sh,ssta[i,:,:]) \n",
    "    plt.title('SST anomaly map: '+str(years[i]))\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42bbb3a-d3cb-4b11-b635-2ec56ee8937f",
   "metadata": {},
   "source": [
    "## Arange Data in a form useful for EOF Analysis and apply land mask\n",
    "\n",
    "We need an array that is 2-D with all the data points in one direction and all the months in another\n",
    "\n",
    "Here I double check each point is not land, as the original dataset does a poor job assinging land values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b50a79-e668-4515-8836-26d74f78bda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set mnx, which defines the number of timesteps\n",
    "mnx = len(years)\n",
    "\n",
    "# Convert to numpy\n",
    "sstx = xr.DataArray.to_numpy(ssta)\n",
    "\n",
    "# Need to deal with the nan values that represent no land.\n",
    "\n",
    "# first convert lon to -180 to 180 degrees format\n",
    "lon_sh_corr=[]\n",
    "for i in xr.DataArray.to_numpy(lon_sh):\n",
    "    if i<180:\n",
    "        lon_sh_corr.append(i)\n",
    "    else:\n",
    "        lon_sh_corr.append((360-i)*-1)\n",
    "\n",
    "# now loop htrough and check each point with global land mask package\n",
    "icnt = 0\n",
    "jcnt = 0 \n",
    "landcnt=0\n",
    "for i in xr.DataArray.to_numpy(lat_sh):\n",
    "    jcnt=0\n",
    "    for j in lon_sh_corr:\n",
    "        if ~globe.is_ocean(i,j):\n",
    "\n",
    "            for k in range(mnx):\n",
    "                sstx[k,icnt,jcnt] = np.nan\n",
    "            landcnt +=1\n",
    "        jcnt+=1\n",
    "    icnt+=1\n",
    "\n",
    "# return number of conflicts found\n",
    "print(\"found \",landcnt,\"land conflicts\")\n",
    "\n",
    "# convert into format for xarray\n",
    "sstx = np.reshape(sstx,(mnx,sstx.shape[1]*sstx.shape[2]))\n",
    "\n",
    "# start by computing a nan mask\n",
    "nan_mask = np.isnan(sstx[0,:])\n",
    "# now remove NaNs\n",
    "sstx = sstx[:,~nan_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d34e0f-cca6-4da5-be5e-db7233283cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to check on the land mask. Should show higher resolution land than the last plot\n",
    "\n",
    "if True:\n",
    "\n",
    "    # pick arbitrary month\n",
    "    i = 150\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.contourf(lon_sh,lat_sh,ssta[i,:,:]) \n",
    "    plt.title('SST anomaly map: '+str(years[i]))\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456f4fd-a33f-4aca-875a-0c9008334613",
   "metadata": {},
   "source": [
    "## Now we can check autocorrelation and estimate decrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a7abd-f627-4603-8f4d-e51b998d66eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ok, now I think we have an array sstz that is [months,space]  Where only land regions are kept\n",
    "# First let's estimate the number of degrees of freedom in the sample by estimating a grand autocorrelation\n",
    "def autocorr2(x,lags):\n",
    "    '''manualy compute, non partial'''\n",
    "\n",
    "    mean=np.nanmean(x)\n",
    "    var=np.nanvar(x)\n",
    "    if var == 0.0:\n",
    "        var=1.0e-6\n",
    "    xp=x-mean\n",
    "    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])/len(x)/var for l in lags]\n",
    "    #corr=[ np.sum(xp[l:]*xp[:-l])/len(x)/var for l in lags]\n",
    "    return np.array(corr)\n",
    "xcor = np.zeros([2])\n",
    "\n",
    "nt, ns = np.shape(sstx)\n",
    "print('nt',nt,'ns',ns)\n",
    "type(ns)\n",
    "\n",
    "lags = [0,1]\n",
    "for i in range(ns):\n",
    "    xx=np.squeeze(sstx[:,i])\n",
    "    xcor1 = autocorr2(xx,lags)\n",
    "    xcor = xcor + xcor1\n",
    "\n",
    "print('xx',xx[0:4],xx[-4:])    \n",
    "xcor = xcor/float(ns)\n",
    "print('xcor ',xcor)\n",
    "acorr = xcor[1]\n",
    "dof_sst = mnx*(1. - acorr**2)/(1. + acorr**2)\n",
    "print('DOF = ',dof_sst,',   DOF/mnx = ',dof_sst/mnx,',  mnx = ',mnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0773c0-22e6-4702-b9a5-566fda95d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sstx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8618b5d-92eb-445b-897f-2d05bf93cc9f",
   "metadata": {},
   "source": [
    "## Finally - we can attempt EOF analysis\n",
    "\n",
    "We're starting this step with sst x, which is lax times lon wide and monthstimes years tall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dbc57-e070-4fea-9b3d-260f10c9e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape again\n",
    "print('sstx shape ',np.shape(sstx))\n",
    "\n",
    "# do some matrix math!\n",
    "mmax = np.max(sstx)\n",
    "print('mmax ',mmax)\n",
    "u, s, vh = np.linalg.svd(sstx.T,full_matrices=False)\n",
    "print('u shape',np.shape(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af3584-1732-4729-8801-49f8d16cd03c",
   "metadata": {},
   "source": [
    "## Plot Eigenvalues of EOF / Princial Component Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b917d2-2f65-45a0-a1c9-90d02711222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('u shape',np.shape(u))\n",
    "print('vh shape',np.shape(vh))\n",
    "print('s shape',np.shape(s))\n",
    "type(u)\n",
    "type(vh)\n",
    "type(s)\n",
    "spectrum = s*s.T\n",
    "spectrum = spectrum/sum(spectrum)\n",
    "plt.figure(figsize=(12, 4), dpi=100)\n",
    "yerror = spectrum*np.sqrt(2/dof_sst)\n",
    "index = np.linspace(0,24,25)\n",
    "    \n",
    "plt.errorbar(index[0:12],spectrum[0:12],yerror[0:12],capsize=5)\n",
    "plt.ylabel('Fraction of Variance')\n",
    "plt.xlabel('EOF number')\n",
    "plt.title('Eigenvalue Spectrum - ' + str(lon1) + '-' + str(lon2) + '  ' + str(lat2) +'-' \\\n",
    "              + str(lat1)  )       \n",
    "#  We need to construct the EOF map by regressing the pc onto the original data\n",
    "pcmx=4  # were going to consider the first 4 eofs\n",
    "\n",
    "ts = vh[0:pcmx,:]  # hope this is time series of first eof, YES looks right, has autocorrelation\n",
    "\n",
    "for pci in range(0,pcmx):\n",
    "\n",
    "    plt.figure(figsize=(12, 4), dpi=100)\n",
    "    plt.plot(years,ts[pci,:])\n",
    "    plt.title('Timeseries of PC-' + str(pci+1) +' - ' + str(lon1) + '-' + str(lon2) + '  ' + str(lat2) +'-' \\\n",
    "              + str(lat1)  )\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5a476-9372-4122-af69-90ba7b79a3c1",
   "metadata": {},
   "source": [
    "## Regress onto the orignal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242eba2-1db3-4780-bd4f-3423003b3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty regression vector\n",
    "regm = np.empty([pcmx+1,latx*lonx])\n",
    "\n",
    "# Loop through each PC\n",
    "for pci in range(0,pcmx):\n",
    "\n",
    "    # extract the t vector for this PC\n",
    "    t=ts[pci,:]\n",
    "\n",
    "    # normalize predictor to have one standard deviation\n",
    "    t = t/np.std(t)  \n",
    "\n",
    "    # extract original data, and convert to 2D matrix\n",
    "    sstb = xr.DataArray.to_numpy(sst0)\n",
    "    sstb=np.reshape(sstb,(mnx,latx*lonx))\n",
    "\n",
    "    # compute regression (t*ssb)/mnx\n",
    "    reg = np.matmul(t,sstb)/mnx\n",
    "\n",
    "    # save this regression\n",
    "    regm[pci,:] = reg\n",
    "    \n",
    "regm = np.reshape(regm,(pcmx+1,latx,lonx))  # reshape for plotting    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8cdb6-e283-4a77-b902-884db1786af5",
   "metadata": {},
   "source": [
    "## Plot regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7774b-3cb1-4e51-b6cb-a74fd8e5cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour plot some regressions\n",
    "\n",
    "# set colormaps\n",
    "col_mapc = 'RdYlBu_r'\n",
    "col_mapr = 'RdYlBu_r'\n",
    "\n",
    "# Loop through each PC\n",
    "for ip in range(0,pcmx):\n",
    "\n",
    "    # find appropreate regression for this PC\n",
    "    regm1 = regm[ip,:,:]\n",
    "    \n",
    "    # here I am finding the maximum value, then I will fix the contours and colorbar to be constant across months\n",
    "    if ip == 0:  # same contour interval for all plots\n",
    "        rmax = np.nanmax(np.abs(regm1))\n",
    "        nconts=60\n",
    "        contr = np.linspace(-rmax,rmax,nconts+1)\n",
    "        print(',  rmax ',rmax, ' - ',np.max(contr))\n",
    "\n",
    "    # toggle True/False to plot maps of regressions\n",
    "    if True:\n",
    "        \n",
    "        # here we extend the array by one, so there is no gap in the contours\n",
    "        prcpmp = np.empty([latx,lonx+1])  \n",
    "        prcpm=regm1\n",
    "        prcpmp[:,0:len(lon)]= prcpm\n",
    "        prcpmp[:,len(lon+1)]= prcpm[:,0]\n",
    "\n",
    "        # make figure\n",
    "        plt.figure(figsize=(12, 4), dpi=100)\n",
    "\n",
    "        # set up axes\n",
    "        ax = plt.axes(projection=ccrs.Robinson(central_longitude=cent_lon))\n",
    "\n",
    "        # Plot data\n",
    "        plt.contourf(lonp, lat, prcpmp, contr, cmap=col_mapr,\n",
    "             transform=ccrs.PlateCarree())\n",
    "\n",
    "        # add gridlines\n",
    "        gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,\n",
    "                  linewidth=1.5, color='gray', alpha=0.5, linestyle='-')\n",
    "\n",
    "        # housekeeping\n",
    "        ax.coastlines()\n",
    "        plt.colorbar()\n",
    "        plt.title('Regression for EOF ' + str(ip+1) + ' \\N{DEGREE SIGN}C ' + str(yra) + '-' + str(yrb) + '  ' + str(lon1) + '-' + str(lon2) + '  ' + str(lat2) +'-' \\\n",
    "              + str(lat1) )\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764833b3-31f9-46ef-a0e1-2968be3f0e18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f3ca2-d3f2-4c45-a41e-5c840f3f580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sstx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1129cbc-4008-48d9-a6ee-0489a16c8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "1488/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96d93c-5ad4-45db-9dfd-14ce78fff7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
